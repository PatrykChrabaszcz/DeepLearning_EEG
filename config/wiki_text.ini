[experiment]
model_class_name = WikiTextRNN
reader_class_name = WikiTextReader
config_file =
verbose = 1

[model]
rnn_hidden_size = 1024
embedding_size = 1024
rnn_num_layers = 3
dropout_f = 0.3
dropout_h = 0.3
rnn_cell_type = GRU
use_context = 0
batch_norm = 0
skip_mode = none

[data_reader]
data_path = /home/chrabasp/data/WikiText
readers_count = 4
batch_size = 64
sequence_size = 20
# Pass full examples for validation, but we need to decrease batch_size
validation_sequence_size = 100
validation_batch_size = 1

balanced = 0
random_mode = 2
continuous = 1
limit_examples = 0
limit_duration = 0
forget_state = 0

[model_trainer]
budget = 1000
metrics_class = SimpleLossMetrics
budget_type = iteration
lr = 0.001
l2_decay = 0.0
objective_type = CrossEntropy_last
cosine_decay = 1
optimizer = AdamW

[bayesian_optimizer]
n_iterations = 100
eta = 3
min_budget = 1
max_budget = 27

[config_generator]
min_points_in_model = 10
top_n_percent = 15
num_samples = 27
random_fraction = 0.2
bandwidth_factor = 3