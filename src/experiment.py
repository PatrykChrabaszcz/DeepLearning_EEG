from src.dl_core.dl_worker import DLWorker
from src.experiment_arguments import ExperimentArguments
import logging
import src.data_reading as reader_module
from src.utils import setup_logging
from src.bayesian_opt.bayesian_optimizer import BayesianOptimizer
from src.bayesian_opt.config_generator import ConfigGenerator
from src.budget_decoder import SimpleBudgetDecoder


# Initialize logging
logger = logging.getLogger(__name__)


class Experiment:
    @staticmethod
    # Parsing arguments
    def add_arguments(parser):
        parser.add_argument("--config_file", type=str, default='',
                            help="File with .pcs (parameter configuration space) data.")
        parser.add_argument("--model_class", default='SimpleRNN',
                            help="Model class used for the training.")
        parser.add_argument("--reader_class", default='AnomalyDataReader',
                            help="Reader class used for the training.")
        parser.add_argument("--backend", default="Pytorch",
                            help="Whether to use Tensorflow or Pytorch.")
        parser.add_argument("--verbose", type=int, default=0, choices=[0, 1],
                            help="If set to 1 then log debug messages.")
        parser.add_argument("--is_master", type=int, default=0, choices=[0, 1],
                            help="If set to 1 then it will run thread for BO optimization.")
        return parser

    def __init__(self):
        # Parse initial experiment arguments
        initial_arguments = ExperimentArguments(use_all_cli_args=False)
        initial_arguments.add_class_arguments(Experiment)
        initial_arguments.get_arguments(sections=('experiment',))

        verbose = initial_arguments.verbose
        setup_logging(logging.DEBUG if verbose else logging.INFO)

        self.is_master = initial_arguments.is_master

        backend = initial_arguments.backend.title()
        # Initialize backend (Currently only PyTorch implemented)
        if backend == 'Tensorflow':
            logger.warning('Tensorflow backend is outdated.')
            import src.dl_tensorflow.model as model_module
            from src.dl_tensorflow.model_trainer import ModelTrainer as TrainerClass
        elif backend == 'Pytorch':
            logger.info('Will use PyTorch backend.')
            import src.dl_pytorch.model as model_module
            from src.dl_pytorch.model_trainer import ModelTrainer as TrainerClass
        else:
            raise NotImplementedError('Specify backend as Tensorflow or PyTorch')

        self.ModelClass = getattr(model_module, initial_arguments.model_class)
        self.ReaderClass = getattr(reader_module, initial_arguments.reader_class)
        self.TrainerClass = TrainerClass

        # Populate experiment arguments with arguments from specific classes
        self.experiment_arguments = ExperimentArguments(use_all_cli_args=True)
        self.experiment_arguments.add_class_arguments(Experiment)
        self.experiment_arguments.add_class_arguments(self.ModelClass)
        self.experiment_arguments.add_class_arguments(self.ReaderClass)
        self.experiment_arguments.add_class_arguments(self.TrainerClass)
        self.experiment_arguments.add_class_arguments(BayesianOptimizer)
        self.experiment_arguments.add_class_arguments(ConfigGenerator)
        self.experiment_arguments.get_arguments()

        # Will be initialized if this node is Master Node
        self.config_space = ExperimentArguments.read_configuration_space(initial_arguments.config_file)

    def run_search_experiment(self):
        """
        If is_master is 1 then runs Hyper-Parameter Search optimizer in separate thread

        Runs one worker that will read configurations generated by Hyper-Parameter Search optimizer
        And report results

        :return:
        """
        if self.is_master:
            # Manage conf file creation and removal
            with BayesianOptimizer(self.config_space, **self.experiment_arguments.get_arguments()) as optimizer:
                self.start_worker()
                try:
                    logger.info('Starting Master')
                    optimizer.run()
                except (Exception, KeyboardInterrupt):
                    logger.info('Shutting down parallel workers.')
                    optimizer.shutdown()
        else:
            self.start_worker()

    def start_worker(self):
        budget_decoder = SimpleBudgetDecoder()
        logger.info('Starting Worker')
        worker = DLWorker(self.ModelClass, self.ReaderClass, self.TrainerClass, budget_decoder,
                          self.experiment_arguments, **self.experiment_arguments.get_arguments())
        worker.run(background=True if self.is_master else False)

    def run_one_experiment(self):
        """
        Simply trains one neural network using provided parameters
        """
